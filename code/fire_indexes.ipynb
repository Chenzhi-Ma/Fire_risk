{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990c1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "current_path = os.getcwd()\n",
    "root_path = os.path.dirname(current_path)\n",
    "data_path= root_path + 'data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908285c4",
   "metadata": {},
   "source": [
    "# fire outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195fdf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/df_all_events_with_outcome.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_new\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_pickle(data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_all_events_with_outcome.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    190\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    192\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    193\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/df_all_events_with_outcome.pkl'"
     ]
    }
   ],
   "source": [
    "df_new=pd.read_pickle(data_path + 'df_all_events_with_outcome.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_zip_mapping=pd.read_excel(data_path + '/zip_cbsa_122022.xlsx',dtype=str,sheet_name='ZIP_CBSA_122022')\n",
    "unique_zip =city_zip_mapping[['ZIP', 'CBSA','RES_RATIO']].drop_duplicates()\n",
    "df_new = df_new.merge(unique_zip, left_on=['ZIP5'],right_on=['ZIP'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21196e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_0 = ['PROP_LOSS', 'PROP_VAL', 'CONT_LOSS', 'CONT_VAL','FF_INJ', 'FF_DEATH', 'OTH_INJ', 'OTH_DEATH']\n",
    "df_new[nan_0] = df_new[nan_0].fillna(0)\n",
    "\n",
    "\n",
    "df_new['total_loss'] = df_new['PROP_LOSS'].astype(int) + df_new['CONT_LOSS'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f88ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(750772.9999999999)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loss=df_new.copy()\n",
    "df_loss = df_loss[df_loss['total_loss']>0]\n",
    "threshold = df_loss['total_loss'].quantile(0.99)\n",
    "df_loss = df_loss[df_loss['total_loss'] < threshold]\n",
    "\n",
    "cpi_map={'2012':1,\n",
    "         '2013':1.02,\n",
    "         '2014':1.02,\n",
    "         '2015':1.03,\n",
    "         '2016':1.05,\n",
    "         '2017':1.07,\n",
    "         '2018':1.09,\n",
    "         '2019':1.12,\n",
    "         '2020':1.13,\n",
    "         '2021':1.21,\n",
    "         '2022':1.29\n",
    "}\n",
    "df_loss['total_loss'] = df_loss.apply(lambda row: row['total_loss'] / cpi_map.get(row['accident_year'], 1)*1.29, axis=1)\n",
    "\n",
    "df_loss['total_loss']= df_loss['total_loss'].astype(int)* df_loss['RES_RATIO'].astype(float)\n",
    "\n",
    "df_loss['num_event']=df_loss['RES_RATIO'].astype(float)\n",
    "\n",
    "df_loss_grouped = df_loss.groupby('CBSA').agg({\n",
    "    'num_event': 'sum',\n",
    "    'total_loss': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_loss_grouped['ave_loss_per_event'] = df_loss_grouped['total_loss'] / df_loss_grouped['num_event']\n",
    "df_loss_grouped['num_event'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd1591aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cbsa_risk_proportion(df_consequence, consequence_type, city_zip_mapping):\n",
    "    # Count number of events in each Risk_Group per ZIP5\n",
    "    zip_risk_counts = df_consequence.groupby(['ZIP5', 'Risk_Group']).size().reset_index(name='count')\n",
    "\n",
    "    # Merge ZIP-level counts with city-ZIP mapping\n",
    "    zip_risk_props = zip_risk_counts.rename(columns={'ZIP5': 'ZIP'})\n",
    "    zip_risk_with_cbsa = pd.merge(zip_risk_props, city_zip_mapping[['ZIP', 'CBSA', 'RES_RATIO']], on='ZIP', how='left')\n",
    "\n",
    "    # Adjust counts using RES_RATIO\n",
    "    zip_risk_with_cbsa['adj_count'] = zip_risk_with_cbsa['count'] * zip_risk_with_cbsa['RES_RATIO'].astype(float)\n",
    "\n",
    "    # Aggregate adjusted count at CBSA-Risk_Group level\n",
    "    cbsa_risk_props = zip_risk_with_cbsa.groupby(['CBSA', 'Risk_Group'])['adj_count'].sum().reset_index()\n",
    "\n",
    "    # Total adjusted count per CBSA\n",
    "    cbsa_total = cbsa_risk_props.groupby('CBSA')['adj_count'].sum().reset_index(name='total_adj_count')\n",
    "\n",
    "    # Merge and compute proportion\n",
    "    df_cbsa_risk = pd.merge(cbsa_risk_props, cbsa_total, on='CBSA')\n",
    "    df_cbsa_risk['risk_proportion'] = df_cbsa_risk['adj_count'] / df_cbsa_risk['total_adj_count']\n",
    "    \n",
    "    # Add consequence type\n",
    "    df_cbsa_risk['consequence_type'] = consequence_type\n",
    "    \n",
    "    return df_cbsa_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dfe132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs=df_new.copy()\n",
    "df_fs = df_fs[df_fs['FIRE_SPRD'].notna()]\n",
    "df_fs['Risk_Group'] = df_fs['FIRE_SPRD']\n",
    "\n",
    "df_fs_grouped = compute_cbsa_risk_proportion(df_fs, 'spread', city_zip_mapping)\n",
    "\n",
    "df_fs_grouped=df_fs_grouped[df_fs_grouped['Risk_Group']=='2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a29d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(48647.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inj_specific=pd.read_pickle(data_path + 'inj_specific.pkl')\n",
    "df_inj=df_new.copy()\n",
    "df_inj=df_inj.merge(inj_specific, on='INCIDENT_KEY', how='inner')\n",
    "vsl_proportion = [x * 1000 for x in [0.003, 0.047, 0.266, 0.593, 1]]\n",
    "\n",
    "df_inj['vsl_proportion'] = (\n",
    "    df_inj['SEV_1'] * vsl_proportion[0] +\n",
    "    df_inj['SEV_2'] * vsl_proportion[1] +\n",
    "    df_inj['SEV_3'] * vsl_proportion[2] +\n",
    "    df_inj['SEV_4'] * vsl_proportion[3] +\n",
    "    df_inj['SEV_5'] * vsl_proportion[4]\n",
    ")\n",
    "\n",
    "df_inj['num_event']=df_inj['RES_RATIO'].astype(float)\n",
    "\n",
    "df_inj_grouped = df_inj.groupby('CBSA').agg({\n",
    "    'num_event': 'sum',\n",
    "    'vsl_proportion': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_inj_grouped['ave_vsl_per_event'] = df_inj_grouped['vsl_proportion'] / df_inj_grouped['num_event']\n",
    "df_inj_grouped['ave_vsl_per_event']=df_inj_grouped['ave_vsl_per_event'].astype(float)\n",
    "df_inj_grouped['num_event'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9308bc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CBSA', 'num_event', 'vsl_proportion', 'ave_vsl_per_event'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inj_grouped.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5fb4b",
   "metadata": {},
   "source": [
    "# combine ignition rate and three consequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e46823",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsa_summary=pd.read_csv(data_path + '/cbsa_fire_rate_summary.txt', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc6a41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fire_matrix=cbsa_summary.merge(df_fs_grouped[['CBSA','risk_proportion',]], on='CBSA', how='left')\n",
    "final_fire_matrix=final_fire_matrix.merge(df_loss_grouped[['CBSA', 'num_event', 'total_loss', 'ave_loss_per_event']], on='CBSA', how='left')\n",
    "final_fire_matrix=final_fire_matrix.merge(df_inj_grouped[['CBSA','num_event', 'vsl_proportion', 'ave_vsl_per_event']], on='CBSA', how='left',suffixes=('_loss','_inj'))\n",
    "final_fire_matrix.rename(columns={'risk_proportion': 'fire_spread_2'}, inplace=True)\n",
    "final_fire_matrix['fs_larger2']=1-final_fire_matrix['fire_spread_2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde43392",
   "metadata": {},
   "source": [
    "# calculate ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66514e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsa_county_mapping = pd.read_excel(data_path + '/list1_2023.xlsx', header=None, dtype=str)\n",
    "\n",
    "# Set column names using the second row\n",
    "cbsa_county_mapping.columns = cbsa_county_mapping.iloc[2]\n",
    "\n",
    "# Drop the first two rows (original header and the new column names row)\n",
    "cbsa_county_mapping = cbsa_county_mapping.drop(index=[0, 1,2]).reset_index(drop=True)\n",
    "\n",
    "cbsa_name= cbsa_county_mapping[['CBSA Code', 'CBSA Title',\n",
    "       'Metropolitan/Micropolitan Statistical Area',]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fire_matrix=final_fire_matrix.merge(cbsa_name, left_on='CBSA', right_on='CBSA Code', how='left')\n",
    "final_fire_matrix['adj_population'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057817b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_fire_matrix=final_fire_matrix[final_fire_matrix['adj_population'] >=100_000]\n",
    "final_fire_matrix=final_fire_matrix.dropna()\n",
    "\n",
    "final_fire_matrix = final_fire_matrix.reset_index(drop=True)\n",
    "\n",
    "final_fire_matrix['rank_ign'] = final_fire_matrix['fire_rate_per_100_000_year'].rank(ascending=False, method='min')\n",
    "final_fire_matrix['rank_ave_loss_per_event'] = final_fire_matrix['ave_loss_per_event'].rank(ascending=False, method='min')\n",
    "final_fire_matrix['rank_ave_vsl_per_event'] = final_fire_matrix['ave_vsl_per_event'].rank(ascending=False, method='min')\n",
    "final_fire_matrix['rank_fs_larger2'] = final_fire_matrix['fs_larger2'].rank(ascending=False, method='min')\n",
    "\n",
    "\n",
    "num_rank=10\n",
    "\n",
    "\n",
    "# smaller, higher risk\n",
    "final_fire_matrix['pop_rank'] = (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['rank_pop'],\n",
    "        q=num_rank,\n",
    "        labels=False\n",
    "    ).astype(int) + 1\n",
    ")\n",
    "\n",
    "final_fire_matrix['spread_rank'] = (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['rank_fs_larger2'],\n",
    "        q=num_rank,\n",
    "        labels=False\n",
    "    ).astype(int) + 1\n",
    ")\n",
    "\n",
    "final_fire_matrix['rate_rank'] = (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['fire_rate_per_100_000_year'],\n",
    "        q=num_rank,\n",
    "        labels=False\n",
    "    ).astype(int) + 1\n",
    ")\n",
    "\n",
    "final_fire_matrix['loss_rank'] = (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['rank_ave_loss_per_event'],\n",
    "        q=num_rank,\n",
    "        labels=False\n",
    "    ).astype(int) + 1\n",
    ")\n",
    "\n",
    "final_fire_matrix['inj_rank'] = (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['rank_ave_vsl_per_event'],\n",
    "        q=num_rank,\n",
    "        labels=False\n",
    "    ).astype(int) + 1       \n",
    ")\n",
    "\n",
    "final_fire_matrix['outcome_rank'] = final_fire_matrix['inj_rank'] + final_fire_matrix['spread_rank'] + final_fire_matrix['loss_rank']\n",
    "\n",
    "# large value higher risk\n",
    "\n",
    "\n",
    "epsilon = 1e-6  # to avoid zero\n",
    "\n",
    "min_val = final_fire_matrix['outcome_rank'].min()\n",
    "max_val = final_fire_matrix['outcome_rank'].max()\n",
    "\n",
    "# Reverse scaling and avoid 0\n",
    "final_fire_matrix['outcome_rank_scaled'] = (\n",
    "    (max_val - final_fire_matrix['outcome_rank']) /\n",
    "    (max_val - min_val + epsilon)\n",
    ") * (1 - epsilon) + epsilon\n",
    "\n",
    "\n",
    "\n",
    "epsilon = 1e-6  # to avoid zero\n",
    "\n",
    "min_val = final_fire_matrix['rank_ign'].min()\n",
    "max_val = final_fire_matrix['rank_ign'].max()\n",
    "\n",
    "final_fire_matrix['ign_rank_scaled']  = (\n",
    "    (max_val - final_fire_matrix['rank_ign']) /\n",
    "    (max_val - min_val + epsilon)\n",
    ") * (1 - epsilon) + epsilon\n",
    "\n",
    "final_fire_matrix['total_fire_risk'] = final_fire_matrix['ign_rank_scaled']*final_fire_matrix['outcome_rank_scaled']\n",
    "\n",
    "final_fire_matrix['overall_rank'] =  (\n",
    "    pd.qcut(\n",
    "        final_fire_matrix['total_fire_risk'],\n",
    "        q=5,\n",
    "        labels=False\n",
    "    ).astype(int) + 1       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c025d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
